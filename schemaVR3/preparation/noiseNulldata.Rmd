---
title: "Effect of noise in null data"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


library(ggplot2)
library(cowplot)
library(BayesFactor)
```

# Outset of the problem

In a recent discussion for a Bayesian design analysis it came up whether noise in data generated under the null hypothesis ($\mu = 0$) affects the simulation. Even though it is counter-intuitive it should not. This becomes clear if we think about the effect size Cohen's D. For an one-sample t-test compared against zero, it is the mean divided by the SD:

$$Cohen's D = \frac{\mu}{\sigma}$$ 

If $\mu = 0$ then it does not matter what value $\sigma$ is and this should be true for both frequentist and Bayesian versions of the t-tests.

# Simulation
Below, I simulate two data sets with a mean of 0: one with a SD of 1 and another with a SD of 100. Both data sets should result in the same distribution of the test statistics, p-values and BFs. 

```{r, eval = FALSE}
# This script simulates null data with two different SDs

# Libraries
library(BayesFactor)
library(parallel)

# Parameters
n  <- 10000000
s1 <- 1
s2 <- 100

# Generating the data sets
data1 <- matrix(rnorm(n, 0, s1), ncol = 100)
data2 <- matrix(rnorm(n, 0, s2), ncol = 100)


# Functions
fun1 <- function(x){
  return(t.test(x)$statistic)
}

fun2 <- function(x){
  return(t.test(x)$p.value)
}

fun3 <- function(x){
  return(as.numeric(as.vector(ttestBF(x))))
}

datedFileNam <- function(fileName, fileEnding){
  return(paste(fileName, 
               '_',
               format(Sys.time(), '%Y%m%d_%H%M%S'), 
               fileEnding,
               sep = ''))
}

# Extracting values
# T-values
t1   <- apply(data1, 1, fun1)
t2   <- apply(data2, 1, fun1)

# p-values
p1   <- apply(data1, 1, fun2)
p2   <- apply(data2, 1, fun2)

# BFs
bf1  <- apply(data1, 1, fun3)
bf2  <- apply(data2, 1, fun3)

# Concatenating to data frame
df1  <- data.frame(`p-value` = p1,
                   `t-value` = t1,
                   BF = bf1) 

df2  <- data.frame(`p-value` = p2,
                   `t-value` = t2,
                   BF = bf2) 

# Creating cluster
numCores <- detectCores() - 1
print(paste('Cores used:', numCores))
cluster  <- makeCluster(numCores)
clusterExport(cluster, c('ttestBF',
                         't.test'))

# Run simulation
# t-values
t1  <- parApply(cluster, data1, 1, fun1)
t2  <- parApply(cluster, data2, 1, fun1)

# p-values
p1  <- parApply(cluster, data1, 1, fun2)
p2  <- parApply(cluster, data2, 1, fun2)

# BF
bf1 <- parApply(cluster, data1, 1, fun3)
bf2 <- parApply(cluster, data2, 1, fun3)

# Concatenating to data frame
df1  <- data.frame(`p-value` = p1,
                   `t-value` = t1,
                   BF = bf1) 

df2  <- data.frame(`p-value` = p2,
                   `t-value` = t2,
                   BF = bf2) 

# Stopping analysis
stopCluster(cluster)

# Saving results
save.image(file = datedFileNam('nullDataNoise', '.RData'))

```

If the test statistics are now compared. 

